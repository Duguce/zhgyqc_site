<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data analysis on Yu Ching San</title>
    <link>https://zhgyqc.netlify.app/categories/data-analysis/</link>
    <description>Recent content in data analysis on Yu Ching San</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 19 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://zhgyqc.netlify.app/categories/data-analysis/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A/B试验概述</title>
      <link>https://zhgyqc.netlify.app/cn/2023/06/19/ab-test/</link>
      <pubDate>Mon, 19 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://zhgyqc.netlify.app/cn/2023/06/19/ab-test/</guid>
      <description>引言 本文主要分享A/B试验的相关概念。我将主要从以下几个方面对A/B试验进行分享：A/B试验的基本原理、实验流程、构用场景和常见问题等。
基本原理 A/B试验是一种常用的实验设计方法，本质是一种对比分析方法。A/B试验属于实验范畴，通过对用户进行随机分组，根据单一变量的原则为每一组用户分配一个实验方案，在相同的时间维度观察用户的反应以确定最佳方案。
以网页皮肤优化为例，原方案A中网页颜色为绿色，调整后的方案B网页颜色为黄色。为了确定用户对网页颜色的偏好，可以设计A/B试验，将特征相似的用户随机分成A、B两组，让其分别打开不同颜色的网页，观察两组用户的网页点击率。
A/B试验之所以能够在数据分析领域得到广泛应用，是因为它能够解决大多数关于选择的问题，通过A/B试验选择出来的方案大部分情况下会使得投入产出比最大化。总之，A/B试验可以辅助业务方选出最优方案并在现有流量中获得更高的投入产出比。
评估方案优劣，选择最优方案 A/B试验的主要目的之一是判断两个方案中哪个更好，以辅助运营人员选择最优方案，达到最好的效果。以网页皮肤选择为例，通过A/B试验确定用户对网页皮肤的偏好，以提升用户点击率，降低用户跳出率。
计算ROI，提升收益 通过A/B试验选择最优方案的终极目的还是提升收益、量化收益，计算投入产出比（ROI）也是数据分析师需要掌握的技能。对于不同的方案而言，成本都是可以直接计算出来的；而对于收益，则需要计算试验组与对照组收益的差值。
实验流程 A/B试验是一个系统性的过程，以用户对网页皮肤的偏好选择为例，其实施流程可以归纳为以下7个步骤：
设定目标 进行A/B试验的第一步是确定比较指标，选取哪些指标进行对比需要根据实验的目的来决定。A/B试验中的指标可以分为三类，即核心指标、辅助指标和反向指标，在进行A/B试验时建议同时选择三类指标作为试验指标。
通过A/B试验确定用户对网页皮肤的偏好，就可以选择点击率作为比较的核心指标。为了排除同一个用户多次点击造成的统计偏差，这里将点击率的统计口径定为去重的点击人数与去重的页面访问人数的比值。
除核心指标外，也需要一些辅助指标和反向指标。辅助指标可以根据用户行为漏斗进行设定；也可以选择重要的下游指标，如平均点击次数、下单成功率、复购率等；反向指标是可能对产品产生负面影响的指标，如回跳率、退货率、回撤率和应用删除率等。
创建变量 选定指标之后，就需要进行变量的创建，即对网页的元素进行更改，将之前绿色的皮肤改为黄色的皮肤。这部分由前端配合完成。
生成假设 有了变量之后，可以基于经验对试验结果做出假设。例如，可以假设用户更喜欢改版后的网页。
确定分流（抽样）方案 如何分配流量、分配多少流量关系到A/B试验的成败，尽管选择同质性较高的用户，也就是各个维度特征较为相似的用户进行测试，同时需要确定分流比例和其他分流细节。
国内外很多开源网站都提供了A/B试验样本量计算器，evanmiller是其中的一种。只需输入目前大盘基准值、预期试验提升效果、置信度及功效等参数，即可计算出试验所需的样本值。目前大盘的基准值为41.68%，预期能够提升的比率为0.2%。如果估计不准，为了保证试验能够得到结果，此处可低估，不可高估，也就是0.2%是预期能够提升的最小值。在95%的置信度、80%的功效下每一组所需的最小样本量为\(95.4138×10^4\)。
如果预期的指标是与均值相关的指标，如人均时长、人均付费等指标，估算样本量可能会稍微复杂。这时候需要运用t检验反算样本量，但同样可以使用各类开源的网页工具进行计算，如字节跳动的DataTester、腾讯的A/B试验平台及百度的峙一平台。
确定试验时长 试验时长也是A/B试验的重要环节，即这个A/B试验要持续多久。试验时长不宜过短，否则参与试验的用户几乎都是活跃用户。试验时间的长短和所需样本量是密切相关的，步骤4中已经估算了所需样本量，因此问题转化成要达到95.4138万个试验样本需要多少天。目前平台每天能为这个A/B试验项目分配10万MB的流量，要达到95.4138万个样本则至少需要10天，这样一来试验时长基本确定。
收集数据 最后就是基于统计学基础理论，分析数据结果，判断两个版本之间是否存在统计学上的显著性差异。
应用场景 A/B试验是一种广泛应用的实验方法，适用于许多领域。以下是几个A/B试验的常见应用场景：
网页设计和用户体验优化：通过A/B试验，可以比较不同网页设计、布局、颜色、按钮位置等因素对用户行为和转化率的影响，从而优化用户体验和提高网站的效果。 营销和广告策略：A/B试验可用于比较不同广告文案、图像、呈现方式、定位或促销策略的效果。例如，测试不同标题、呈现方式、优惠券等，以确定哪种策略对受众产生最佳影响。 产品功能和界面改进：A/B试验可帮助确定产品功能的效果和用户偏好。通过比较不同功能的版本，可以了解用户对产品特性的偏好，并决定是否进行功能改进或调整。 电子商务和购物体验优化：通过A/B试验，可以比较不同购物页面设计、产品推荐策略、付款选项等对购物转化率的影响，以优化电子商务平台的用户体验和销售效果。 移动应用开发和优化：A/B试验可用于比较不同移动应用界面、功能、通知推送等的效果，以改进用户留存率、转化率和用户满意度。 电子邮件营销优化：通过A/B试验，可以测试不同的电子邮件主题、内容、发送时间等因素对打开率、点击率和转化率的影响，以提高电子邮件营销的效果。 社交媒体营销和广告：A/B试验可以用于比较不同社交媒体广告文案、图像、呈现方式和定位等的效果，以优化社交媒体广告投放的效果和投资回报率。 常见问题 A/B试验常见的误区 参考答案
（1）忽略统计学意义是A/B试验常见的误区之一，包括忽视假设检验、显著性水平以及统计功效。 仍以网站皮肤优化为例，若A组的点击率是39.13%，B组的点击率仅为36.86%，是否可以直接说A组的效果优于B组，用户更喜欢A组的皮肤颜色呢？事实上，不能直接得出这个结论，因为缺少了关键步骤——假设检验。假设检验的目的之一是排除运气、抽样误差等随机因素对试验结果的误判，即通常所说的Ⅰ类错误；目的之二是排除由于漏报对于试验结果的影响，即Ⅱ类错误。为了避免Ⅰ类错误、Ⅱ类错误带来的误判和漏报，需要对试验结果进行严格的假设检验，类似于留存率、渗透率等率值相关指标可以采用Z检验或卡方检验（非正态情况下），而人均时长、用户购买量等指标可以使用t检验。
（2）由于新奇效应的存在，试验时长的选择也需要注意。 试验所需的样本量决定了试验的时间长短，为了尽快得出结论是否可以分配较大流量使得试验尽快收集到所需样本量？或者按照正常的流量分配，达到样本量之后立即停止试验？
答案是否定的，面对以上两种情况需要考虑是否因为新奇效应的存在给结果带来了一定的影响。在统计学上，新奇效应也称为均值回归，即随着试验次数的增加，结果往往趋近于均值。在A/B试验中，试验早期用户可能会因为新的改动而产生好奇，从而带来点击率的提升，但是随着试验时间的增加，这个点击率会趋近于用户的真实点击水平。因此，需要等到观测指标平稳之后才能停止试验，以避免新奇效用对于试验结果的影响。如果分配较大的流量在短时间内收集够样本，除存在新奇效应外，还可能受到周内效应的影响，即用户在周内、周末的行为习惯不一致造成试验误差；另外，还有可能存在以偏概全。
（3）A/B试验的核心是用户分群试验，对于用户的选择以偏概全，只选择高频用户也是常见的误区之一。 在流量分配的时候需要保证对照组和试验组的用户具有同时性、同质性、均匀性和唯一性。换句话说就是需要将用户属性相近的用户同时分配到A组或B组中且同时进行试验。即使这样还会遇到一个问题，用户虽然大部分特征是相似的，但其活跃周期可能不尽相同，因此同样可能出现以偏概全的误区。因此，试验时间的选择应该格外注意，切不可为了尽快获得试验结果而分配大量流量，需要考虑到用户群体的全覆盖，这个可以结合样本量估算以及用户流失周期等进行思考。
（4）辛普森悖论也是A/B试验中常见的现象，即在试验过程中流量分割比例改变，从而造成结果错误。 辛普森悖论是指在某个条件下的两组数据，分别讨论时都会满足某种性质，可是一旦合并考虑，却可能导致相反的结论。
（5）多个试验同时进行时，如何设计A/B试验也会存在一定的误区。 针对多个试验同时进行，我们可以通过设计正交试验，解决多个试验同时进行时流量分配的问题。
什么是正交实验？ 参考答案
数据分析师经常会遇到多个活动同时进行A/B试验的情况，那么这个时候你会思考一个问题——别人的试验是否会影响到自己正在进行的试验？如果情人节期间开展了一系列的活动，各个活动都在通过A/B试验测试活动效果。老板肯定想知道每个活动的效果如何？所有活动的叠加效果又如何？
A/B试验的正交试验可以解决你的疑问，也可以解决老板的疑问。在设计正交试验时需要严格遵守两个原则，即正交和互斥。
（1）正交。流量正交可以让业务关联度小的试验有足够的流量同时进行，实现流量的高可用性。正交一般情况下是对于不同试验层来说的，将上一层的流量随机打散到下一场的试验中，使得用户再进入其他试验时时均匀分布的，而不是集中在某一块区域。
（2）互斥。流量互斥可以让关联度较大的试验分开进行，避免相互影响，从而保证结果的可信度。流量互斥一般情况下是对同一试验层来说的，在同一试验层的几个策略中同一用户只能进入一个试验测量中。很多情况下，活动整体的效果并不等于各个子活动叠加效果，有时候子活动之间有着互相放大的作用，使得1+1&amp;gt;2；而有的时候，子活动在本质上是相同的事情，从而使得1+1&amp;lt;2。
因此，要量化一个活动的整体效果时，就需要一个贯穿所有活动的对照组，在A/B试验系统中称为贯穿层。与贯穿层相对应的就是试验层，试验层又可以根据活动需要分为不同的子试验层。
按照上述的框架进行试验设计，虽然试验B1层的流量被复用到试验B2层，也就相当于把B1层的试验效果带到了B2层，但是流量是正交的，即B1层的试验效果随机均匀打散分配到B2层，所以B1层的试验效果对B2层的每一个试验策略的影响都是均衡的，整体上来看别人的试验并不会影响到自己的试验效果。
什么是最小预期提升（MDE）？ 参考答案
最小预期提升，亦指最小检测效应（Minimum Detectable Effect，MDE）。在进行A/B试验之前，需要有一个心理预期，例如，当实验组比对照组至少提升3%的效果时，才认为实验组的方案有实际价值，若没有达到预期提升，即使统计检验时显著的，实验组的方案也不值得被采纳。MDE的大小对样本量计算和实验设计至关重要，如果MDE设置的过小，样本量可能会很大，导致昂贵和耗时的实验。如果MDE设置的过大，可能无法检测到较小的但仍具有实际意义的效应。通常，确定MDE的大小需要综合考虑多个因素，包括业务目标、可行性、预期效应大小、统计功效和显著性水平等。较大的MDE可能需要更小的样本量，但可能会错过较小但仍重要的效应。较小的MDE可能需要更大的样本量，但可以更准确地检测到较小的效应。
如何衡量实验效果？ 参考答案
A/B试验的效果可以通过P值、效应值、最小检测效应来衡量。
（1）显著性水平(1-α)和P值。 显著性水平（Significance Level）和P值是判断试验结果是否具有统计显著性的重要指标。显著性水平将犯Ⅰ类错误的概率控制在一给定的水平下，这个水平就是显著性水平，在此基础上使犯第Ⅱ类错误的概率尽可能小。P值在统计学中用来衡量两样本由随机抽样误差，即犯Ⅰ类错误而产生的差异，只要P值足够小，小到可以忽略，就可以认为两样本之间的差异并不是由抽样误差引起的，而是样本本身就存在的差异。</description>
    </item>
    
  </channel>
</rss>
