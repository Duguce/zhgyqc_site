<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Decision Tree on Qingchen Yu</title><link>https://zhgyqc.vercel.app/tags/decision-tree/</link><description>Recent content in Decision Tree on Qingchen Yu</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 22 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://zhgyqc.vercel.app/tags/decision-tree/index.xml" rel="self" type="application/rss+xml"/><item><title>决策树算法概述</title><link>https://zhgyqc.vercel.app/cn/2023/07/22/decision-tree/</link><pubDate>Sat, 22 Jul 2023 00:00:00 +0000</pubDate><guid>https://zhgyqc.vercel.app/cn/2023/07/22/decision-tree/</guid><description>引言 本文分享一个机器学习算法：决策树算法（Decision Tree），是一种常用的监督学习算法，适用于分类和回归问题。决策树通过递归地划分数据集，选择最佳特征来构建树，并在叶节点做出预测。决策树算法具有简单易懂、可解释性强等优点，但也容易过拟合。本文将从以下几个方面对决策树算法进行分享：首先，我将分享决策树算法的基本理论概念；其次，我将使用Python实现一个实战案例；最后，我将分享一些决策树算法常见的面试问题。
算法概述 你是否玩过二十个问题的游戏，游戏的规则很简单：参与游戏的一方在脑海里想某个事物，其他参与者向他提问问题，只允许提问20个问题，问题的答案也只能用对或错回答。问问题的人通过推断分解，逐步缩小待猜测事物的范围。
决策树的工作原理与20个问题类似，用户输入一系列数据，然后给出游戏的答案。如下图所示是一个决策树的流程图。图中构造了一个假想的邮件分类系统，它首先检测发送邮件域名地址。如果地址为“myEmployer.com”，则将其放在分类“无聊时需要阅读的邮件”中。如果邮件不是来自这个域名，则检查邮件的内容是否包含单词“曲棍球”，如果包含则将邮件归类到“需要及时处理的朋友邮件”，如果不包含则将邮件归类到“无需阅读的垃圾邮件”。
决策树算法的预测分类流程如下：
特征选择。在构建决策树时，首先需要选择用于分裂节点的最佳特征。常用的特征选择方法有信息增益、信息增益比、基尼指数等。这些方法根据特征的不确定性或纯度来衡量特征的重要性。 分裂节点。根据选择的特征，将节点分裂成多个子节点。每个子节点代表一个特征取值，将数据集按照该特征取值进行划分。 递归构建。对于每个子节点，重复执行特征选择和分裂节点的过程，直至满足停止的条件。停止的条件可以是达到最大深度、节点中的数据样本小于预定义阈值或节点的不确定性或纯度低于阈值等。 标记叶子节点。当达到停止条件时，将叶子节点标记为特定的类别标签或预测值。对于分类问题，叶子节点通常表示一个类别。（对于回归问题，叶子节点表示一个数值） 预测。通过将输入样本从根节点开始沿着决策树的路径进行遍历，最终达到叶子节点，从而预测样本的类别。（或数值） 决策树算法的优缺点如下：
优点：
可解释性强。决策树的结构清晰，易于理解和解释。可以通过可视化展示决策树，直观地观察到决策过程和特征的重要性。 处理混合数据类型。决策树可以处理包含离散型和连续型特征的数据，而无需对特征进行特殊的预处理或转换。 对缺失值和异常值鲁棒。决策树能够自然地处理缺失值，并且对异常值相对鲁棒，不会对异常值过度敏感。 计算效率高。相对于其他复杂的机器学习算法，决策树的训练和预测速度较快，尤其适用于处理大型数据集。 缺点：
容易过拟合。决策树容易生成过于复杂的模型，过拟合训练数据，导致在新数据上的泛化能力较差。可以通过剪枝等方法来减轻过拟合问题。 忽略特征间的相关性。决策树基于每个节点上的最佳特征进行分裂，可能忽略特征之间的相关性。在某些情况下，其他算法（如基于统计的方法）能够更好地处理特征之间的复杂关系。 对噪声敏感。当数据中存在噪声时，决策树可能会过度拟合噪声，导致模型不稳定。 实战案例 案例概述 本文使用一组海洋生物数据，需要将这些动物分为两类：鱼类和非鱼类。如下表所示的数据包含5个海洋动物，特征包括：不浮出水面是否可以生存，以及是否有脚蹼。现在需要决定是依据第一个特征还是第二个特征划分数据。本文将使用ID3算法划分数据集，该算法处理如何划分数据集，何时停止划分数据集。 信息增益 划分数据集的大原则是将无序的数据变得更加有序。组织杂乱无章数据的一种方法就是使用信息论度量信息，信息论是量化处理信息的分支科学。我们可以在划分数据之前或之后使用信息论量化度量信息的内容。在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。
集合信息的度量方式称为香农熵或者简称为熵，这个名字来源于信息论之父克劳德·香农。
熵定义为信息的期望值，在明晰这个概念之前，我们必须知道信息的定义。如果待分类的事物可能划分在多个分类之中，则符号$x_i$的信息定义为$l(x_i)=-log_2 p(x_i)$，其中，$p(x_i)$是选择该分类的概率。
为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过以下的公式得到：
$$H=-\sum_{i=1}^{n}p(x_i)log_2 p(x_i)$$
其中，n是分类的数目。
以下是基于Python实现的计算信息熵的代码：
from math import log def calc_shannon_ent(dataset): &amp;#34;&amp;#34;&amp;#34; Calculate Shannon Entropy of a dataset &amp;#34;&amp;#34;&amp;#34; num_entries = len(dataset) # number of entries label_counts = {} # dictionary to store the number of each class # loop over all the entries for feat_vec in dataset: current_label = feat_vec[-1] # get the label of the current entry if current_label not in label_counts.</description></item></channel></rss>