<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>K-Nearest Neighbors on Qingchen Yu</title><link>https://zhgyqc.vercel.app/tags/k-nearest-neighbors/</link><description>Recent content in K-Nearest Neighbors on Qingchen Yu</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 18 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://zhgyqc.vercel.app/tags/k-nearest-neighbors/index.xml" rel="self" type="application/rss+xml"/><item><title>K-近邻算法概述</title><link>https://zhgyqc.vercel.app/cn/2023/06/18/knn/</link><pubDate>Sun, 18 Jun 2023 00:00:00 +0000</pubDate><guid>https://zhgyqc.vercel.app/cn/2023/06/18/knn/</guid><description>引言 本文分享第一个机器学习算法：K-近邻算法（K-Nearest Neighbor, KNN），最初由Cover和Hart于1968年提出，是最简单的机器学习算法之一。本文将从以下几个方面对K-近邻算法进行分享：首先，我将分享K-近邻算法的基本理论概念；其次，本文将使用Python实现一个实战案例；最后，我将分享一些K-近邻算法常见的面试问题。
算法概述 K-近邻算法是通过测量不同特征值之间的距离方法进行分类。它的工作原理是：存在一个样本数据集合，也称为训练样本集，并且样本集中每个数据都存在标签，即已知样本集中每一数据与所属分类的对应关系。当输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前\(k\)个最相似的数据，这就是K-近邻算法中的\(k\)出处，通常是\(k\)不大于20的整数。最后，选择\(k\)个最相似数据中出现次数最多的分类，作为新数据的分类。
K-近邻算法的预测分类流程如下：
计算已知类别数据集中的点与当前点之间的距离； 按照距离递增次序排序； 选取与当前点距离最小的\(k\)个点； 确定前\(k\)个点所在类别的出现频率； 返回前\(k\)个点出现频率最高的类别作为当前点的预测分类。 其中，计算距离一般使用欧氏距离公式：
$$ d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2} $$
K-近邻算法的优缺点如下：
优点：
简单易懂： K-近邻算法是一种直观、简单的算法，容易理解和实现； 无需训练： K-近邻算法不需要训练，它可以直接利用已有的数据进行推断，因此对新数据的适应性较好； 对异常值不敏感： 由于K-近邻算法是基于局部信息进行推断，它对于异常值的影响相对较小，不易受异常样本的干扰。 缺点：
计算开销较大： 在进行预测时，K-近邻算法需要计算新样本与所有训练样本之间的距离，因此在处理大规模数据集时，计算开销较高，对计算资源要求较高； 特征维度问题： 随着特征维度的增加，计算样本之间的距离会变得更加困难，这一问题被称为“唯独灾难”。在高维空间中，样本间的距离往往趋于相等或相差不大，导致K-近邻算法的性能下降； 对不平衡数据集敏感： 当训练数据集中的类别不平衡时，样本较多的类别将对预测结果产生较大的影响，从而导致偏差。这可能会导致K-近邻算法在处理不平衡数据集时的性能下降。 注意： K-近邻算法中的\(k\)值是一个超参数，需要根据具体问题和数据进行调优。选择不合适的\(k\)值可能导致欠拟合或过拟合问题。
实战案例 本文所使用的是一个改进约会网站配对效果的案例，原始案例来自于《机器学习实战》 一书，另外这里推荐一个写的还不错的书中项目对应的源码地址 。
案例概述 我的朋友海伦一直使用在线约会网站寻找适合自己的约会对象。尽管约会网站会推荐不同的人选，但她并不是喜欢每一个人。经过一番总结，她发现曾交往过三种类型的人：
不喜欢的人 魅力一般的人 极具魅力的人 尽管发现了上述规律，但海伦依然无法将约会网站推荐的匹配对象归入恰当的分类。她觉得可以在周一到周五约会那些魅力一般的人，而周末则更喜欢与那些极具魅力的人为伴。海伦希望我们的分类软件可以更好地帮助她将匹配对象划分到确切的分类中。此外海伦还收集了一些约会网站未曾记录的数据信息，她认为这些数据更有助于匹配对象的归类。
这里我们就使用K-近邻算法来实现这一任务，下面是基本流程：
收集数据：提供文本文件； 准备数据：使用Python解析文本文件； 分析数据：使用Matplotlib画二维扩散图； 测试算法：使用海伦提供的部分数据作为测试样例； 使用算法：产生简单的命令行程序，然后海伦可以输入一些特征数据以判断对方是否为自己喜欢的类型。 注意： 测试样本和非测试样本的区别在于：测试样本是已经完成分类的数据，如果预测分类与实际类别不同，则标记为一个错误。
准备数据 海伦收集约会数据已经有了一段时间，她把这些数据存放在文本文件中，每个样本数据占据一行，总共有1000行。海伦的样本主要包含以下3种特征：
每年获得的飞行常客里程数； 玩视频游戏所耗时间百分比； 每周消费的冰淇淋公升数。 在将上述特征数据输入到分类器之前，必须将待处理数据的格式改变为分类器可以接受的格式。下面是使用Python实现的数据读取代码：</description></item></channel></rss>