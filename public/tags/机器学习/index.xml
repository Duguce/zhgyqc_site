<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>机器学习 on Qingchen Yu</title><link>https://zhgyqc.vercel.app/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link><description>Recent content in 机器学习 on Qingchen Yu</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 22 Jul 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://zhgyqc.vercel.app/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml"/><item><title>决策树算法概述</title><link>https://zhgyqc.vercel.app/cn/2023/07/22/decision-tree/</link><pubDate>Sat, 22 Jul 2023 00:00:00 +0000</pubDate><guid>https://zhgyqc.vercel.app/cn/2023/07/22/decision-tree/</guid><description>引言 本文分享第一个机器学习算法：K-近邻算法（K-Nearest Neighbor, KNN），最初由Cover和Hart于1968年提出，是最简单的机器学习算法之一。我将从以下几个方面对K-近邻算法进行分享：首先，我将分享K-近邻算法的基本理论概念；其次，我将使用Python实现一个实战案例；最后，我将分享一些K-近邻算法常见的面试问题。
算法概述 你是否玩过二十个问题的游戏，游戏的规则很简单：参与游戏的一方在脑海里想某个事物，其他参与者向他提问问题，只允许提问20个问题，问题的答案也只能用对或错回答。问问题的人通过推断分解，逐步缩小待猜测事物的范围。
决策树的工作原理与20个问题类似，用户输入一系列数据，然后给出游戏的答案。如下图所示是一个决策树的流程图。图中构造了一个假想的邮件分类系统，它首先检测发送邮件域名地址。如果地址为“myEmployer.com”，则将其放在分类“无聊时需要阅读的邮件”中。如果邮件不是来自这个域名，则检查邮件的内容是否包含单词“曲棍球”，如果包含则将邮件归类到“需要及时处理的朋友邮件”，如果不包含则将邮件归类到“无需阅读的垃圾邮件”。
决策树算法的预测分类流程如下：
特征选择。在构建决策树时，首先需要选择用于分裂节点的最佳特征。常用的特征选择方法有信息增益、信息增益比、基尼指数等。这些方法根据特征的不确定性或纯度来衡量特征的重要性。 分裂节点。根据选择的特征，将节点分裂成多个子节点。每个子节点代表一个特征取值，将数据集按照该特征取值进行划分。 递归构建。对于每个子节点，重复执行特征选择和分裂节点的过程，直至满足停止的条件。停止的条件可以是达到最大深度、节点中的数据样本小于预定义阈值或节点的不确定性或纯度低于阈值等。 标记叶子节点。当达到停止条件时，将叶子节点标记为特定的类别标签或预测值。对于分类问题，叶子节点通常表示一个类别。（对于回归问题，叶子节点表示一个数值） 预测。通过将输入样本从根节点开始沿着决策树的路径进行遍历，最终达到叶子节点，从而预测样本的类别。（或数值） 决策树算法的优缺点如下：
优点：
可解释性强。决策树的结构清晰，易于理解和解释。可以通过可视化展示决策树，直观地观察到决策过程和特征的重要性。 处理混合数据类型。决策树可以处理包含离散型和连续型特征的数据，而无需对特征进行特殊的预处理或转换。 对缺失值和异常值鲁棒。决策树能够自然地处理缺失值，并且对异常值相对鲁棒，不会对异常值过度敏感。 计算效率高。相对于其他复杂的机器学习算法，决策树的训练和预测速度较快，尤其适用于处理大型数据集。 缺点：
容易过拟合。决策树容易生成过于复杂的模型，过拟合训练数据，导致在新数据上的泛化能力较差。可以通过剪枝等方法来减轻过拟合问题。 忽略特征间的相关性。决策树基于每个节点上的最佳特征进行分裂，可能忽略特征之间的相关性。在某些情况下，其他算法（如基于统计的方法）能够更好地处理特征之间的复杂关系。 对噪声敏感。当数据中存在噪声时，决策树可能会过度拟合噪声，导致模型不稳定。 实战案例 案例概述 本文使用一组海洋生物数据，需要将这些动物分为两类：鱼类和非鱼类。如下表所示的数据包含5个海洋动物，特征包括：不浮出水面是否可以生存，以及是否有脚蹼。现在需要决定是依据第一个特征还是第二个特征划分数据。本文将使用ID3算法划分数据集，该算法处理如何划分数据集，何时停止划分数据集。 信息增益 划分数据集的大原则是将无序的数据变得更加有序。组织杂乱无章数据的一种方法就是使用信息论度量信息，信息论是量化处理信息的分支科学。我们可以在划分数据之前或之后使用信息论量化度量信息的内容。在划分数据集之前之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。
集合信息的度量方式称为香农熵或者简称为熵，这个名字来源于信息论之父克劳德·香农。
熵定义为信息的期望值，在明晰这个概念之前，我们必须知道信息的定义。如果待分类的事物可能划分在多个分类之中，则符号$x_i$的信息定义为$l(x_i)=-log_2 p(x_i)$，其中，$p(x_i)$是选择该分类的概率。
为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过以下的公式得到：
$$H=-\sum_{i=1}^{n}p(x_i)log_2 p(x_i)$$
其中，n是分类的数目。
以下是基于Python实现的计算信息熵的代码：
from math import log def calc_shannon_ent(dataset): &amp;#34;&amp;#34;&amp;#34; Calculate Shannon Entropy of a dataset &amp;#34;&amp;#34;&amp;#34; num_entries = len(dataset) # number of entries label_counts = {} # dictionary to store the number of each class # loop over all the entries for feat_vec in dataset: current_label = feat_vec[-1] # get the label of the current entry if current_label not in label_counts.</description></item><item><title>K-近邻算法概述</title><link>https://zhgyqc.vercel.app/cn/2023/06/18/knn/</link><pubDate>Sun, 18 Jun 2023 00:00:00 +0000</pubDate><guid>https://zhgyqc.vercel.app/cn/2023/06/18/knn/</guid><description>引言 本文分享第一个机器学习算法：K-近邻算法（K-Nearest Neighbor, KNN），最初由Cover和Hart于1968年提出，是最简单的机器学习算法之一。本文将从以下几个方面对K-近邻算法进行分享：首先，我将分享K-近邻算法的基本理论概念；其次，本文将使用Python实现一个实战案例；最后，我将分享一些K-近邻算法常见的面试问题。
算法概述 K-近邻算法是通过测量不同特征值之间的距离方法进行分类。它的工作原理是：存在一个样本数据集合，也称为训练样本集，并且样本集中每个数据都存在标签，即已知样本集中每一数据与所属分类的对应关系。当输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前\(k\)个最相似的数据，这就是K-近邻算法中的\(k\)出处，通常是\(k\)不大于20的整数。最后，选择\(k\)个最相似数据中出现次数最多的分类，作为新数据的分类。
K-近邻算法的预测分类流程如下：
计算已知类别数据集中的点与当前点之间的距离； 按照距离递增次序排序； 选取与当前点距离最小的\(k\)个点； 确定前\(k\)个点所在类别的出现频率； 返回前\(k\)个点出现频率最高的类别作为当前点的预测分类。 其中，计算距离一般使用欧氏距离公式：
$$ d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2} $$
K-近邻算法的优缺点如下：
优点：
简单易懂： K-近邻算法是一种直观、简单的算法，容易理解和实现； 无需训练： K-近邻算法不需要训练，它可以直接利用已有的数据进行推断，因此对新数据的适应性较好； 对异常值不敏感： 由于K-近邻算法是基于局部信息进行推断，它对于异常值的影响相对较小，不易受异常样本的干扰。 缺点：
计算开销较大： 在进行预测时，K-近邻算法需要计算新样本与所有训练样本之间的距离，因此在处理大规模数据集时，计算开销较高，对计算资源要求较高； 特征维度问题： 随着特征维度的增加，计算样本之间的距离会变得更加困难，这一问题被称为“唯独灾难”。在高维空间中，样本间的距离往往趋于相等或相差不大，导致K-近邻算法的性能下降； 对不平衡数据集敏感： 当训练数据集中的类别不平衡时，样本较多的类别将对预测结果产生较大的影响，从而导致偏差。这可能会导致K-近邻算法在处理不平衡数据集时的性能下降。 注意： K-近邻算法中的\(k\)值是一个超参数，需要根据具体问题和数据进行调优。选择不合适的\(k\)值可能导致欠拟合或过拟合问题。
实战案例 本文所使用的是一个改进约会网站配对效果的案例，原始案例来自于《机器学习实战》 一书，另外这里推荐一个写的还不错的书中项目对应的源码地址 。
案例概述 我的朋友海伦一直使用在线约会网站寻找适合自己的约会对象。尽管约会网站会推荐不同的人选，但她并不是喜欢每一个人。经过一番总结，她发现曾交往过三种类型的人：
不喜欢的人 魅力一般的人 极具魅力的人 尽管发现了上述规律，但海伦依然无法将约会网站推荐的匹配对象归入恰当的分类。她觉得可以在周一到周五约会那些魅力一般的人，而周末则更喜欢与那些极具魅力的人为伴。海伦希望我们的分类软件可以更好地帮助她将匹配对象划分到确切的分类中。此外海伦还收集了一些约会网站未曾记录的数据信息，她认为这些数据更有助于匹配对象的归类。
这里我们就使用K-近邻算法来实现这一任务，下面是基本流程：
收集数据：提供文本文件； 准备数据：使用Python解析文本文件； 分析数据：使用Matplotlib画二维扩散图； 测试算法：使用海伦提供的部分数据作为测试样例； 使用算法：产生简单的命令行程序，然后海伦可以输入一些特征数据以判断对方是否为自己喜欢的类型。 注意： 测试样本和非测试样本的区别在于：测试样本是已经完成分类的数据，如果预测分类与实际类别不同，则标记为一个错误。
准备数据 海伦收集约会数据已经有了一段时间，她把这些数据存放在文本文件中，每个样本数据占据一行，总共有1000行。海伦的样本主要包含以下3种特征：
每年获得的飞行常客里程数； 玩视频游戏所耗时间百分比； 每周消费的冰淇淋公升数。 在将上述特征数据输入到分类器之前，必须将待处理数据的格式改变为分类器可以接受的格式。下面是使用Python实现的数据读取代码：</description></item></channel></rss>